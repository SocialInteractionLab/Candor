{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a22813",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "This script cleans the raw annotated data, counts number of annotators, and outputs processed annotation data to be merged with CANDOR transcripts.\n",
    "\n",
    "**Authors:** Changyi Zhou, Helen Schmidt  \n",
    "**Python version:** 3.11.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb568c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69541d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data input location\n",
    "input_folder = \"/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/Conversation-Structure/data/raw/full-sample\"\n",
    "# define data output location\n",
    "output_folder = \"/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/Conversation-Structure/data/processed/full-sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a19f23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['full_data_pilot_4:18.csv', 'full_data_s2.csv', 'full_data_s2-5.csv', 'full_data_s2-4.csv', 'full_data_s2-3.csv', 'full_data_s2-2.csv', 'lowqual2.csv', 'lowqual3.csv', 'lowqual1.csv', 'full_data_s1-1.csv', 'full_data_s1-2.csv', 'full_data_s1-3.csv']\n"
     ]
    }
   ],
   "source": [
    "# get files in the input folder from the full sample\n",
    "files = [file for file in os.listdir(input_folder) if file.endswith('.csv')]\n",
    "# check the raw files look correct\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d8e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/Conversation-Structure/data/raw/full-sample/full_data_pilot_4:18.csv\n",
      "/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/Conversation-Structure/data/raw/full-sample/full_data_s2.csv\n",
      "/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/Conversation-Structure/data/raw/full-sample/full_data_s2-5.csv\n",
      "/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/Conversation-Structure/data/raw/full-sample/full_data_s2-4.csv\n",
      "/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/Conversation-Structure/data/raw/full-sample/full_data_s2-3.csv\n",
      "/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/Conversation-Structure/data/raw/full-sample/full_data_s2-2.csv\n",
      "/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/Conversation-Structure/data/raw/full-sample/lowqual2.csv\n",
      "/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/Conversation-Structure/data/raw/full-sample/lowqual3.csv\n",
      "/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/Conversation-Structure/data/raw/full-sample/lowqual1.csv\n",
      "/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/Conversation-Structure/data/raw/full-sample/full_data_s1-1.csv\n",
      "/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/Conversation-Structure/data/raw/full-sample/full_data_s1-2.csv\n",
      "/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/Conversation-Structure/data/raw/full-sample/full_data_s1-3.csv\n",
      "/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/Conversation-Structure/data/raw/full_dense_subset_raw.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7c4b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_data_pilot_4:18.csv - Total submissions: 144\n",
      "full_data_s2.csv - Total submissions: 285\n",
      "full_data_s2-5.csv - Total submissions: 54\n",
      "full_data_s2-4.csv - Total submissions: 123\n",
      "full_data_s2-3.csv - Total submissions: 122\n",
      "full_data_s2-2.csv - Total submissions: 203\n",
      "lowqual2.csv - Total submissions: 26\n",
      "lowqual3.csv - Total submissions: 21\n",
      "lowqual1.csv - Total submissions: 92\n",
      "full_data_s1-1.csv - Total submissions: 208\n",
      "full_data_s1-2.csv - Total submissions: 259\n",
      "full_data_s1-3.csv - Total submissions: 135\n",
      "Total submissions across all data files: 1672\n"
     ]
    }
   ],
   "source": [
    "# create empty data frame\n",
    "dataframes = []\n",
    "# count the values and occurrences of annotations in each data file\n",
    "for file in files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'messages' in df.columns:\n",
    "        messages_df = df[df['messages'].notna() & (df['messages'].str.strip() != '')]\n",
    "        if 'data' in messages_df.columns:\n",
    "            messages_df = messages_df.drop(columns=['data'])\n",
    "\n",
    "        dataframes.append(messages_df)\n",
    "\n",
    "        if 'count' in messages_df.columns:\n",
    "            count_values = messages_df['count'].value_counts()\n",
    "\n",
    "            count_values_df = pd.DataFrame({\n",
    "                'count_value': count_values.index,\n",
    "                'occurrences': count_values.values\n",
    "            })\n",
    "\n",
    "            print (f'{file} - Total submissions: {count_values_df[\"occurrences\"].sum()}')\n",
    "\n",
    "        else:\n",
    "            print(\"The 'count' column was not found in the messages DataFrame.\")\n",
    "\n",
    "raw_combined = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# print final number\n",
    "print (f'Total submissions across all data files: {len(raw_combined)}')\n",
    "# display(raw_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86b4a497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotated transcripts in dense subset: 200\n"
     ]
    }
   ],
   "source": [
    "# get dense subset csv\n",
    "dense_subset = pd.read_csv('/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/Conversation-Structure/data/raw/full_dense_subset_raw.csv')\n",
    "# get list of transcript ids\n",
    "dense_transcripts = dense_subset['transcript_id'].unique()\n",
    "print (f'Total annotated transcripts in dense subset: {len(dense_transcripts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35b04067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing count values: []\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# check that remaining conversations from the CANDOR data set have been annotated by at least one person (N = 1456 unique transcripts; doesn't include dense subset of 200)\n",
    "\n",
    "expected_counts = set(range(1456))  # 0 to 1455 inclusive\n",
    "all_counts = set()\n",
    "\n",
    "for file in os.listdir(input_folder):\n",
    "    if not file.endswith('.csv'):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, usecols=['messages', 'count'])  # only load needed columns\n",
    "    except ValueError:\n",
    "        print(f\"File {file} is missing required columns.\")\n",
    "        continue\n",
    "\n",
    "    # Filter rows with valid messages\n",
    "    filtered = df['messages'].fillna('').str.strip() != ''\n",
    "    counts = df.loc[filtered, 'count'].dropna().astype(int).unique()\n",
    "\n",
    "    all_counts.update(counts)\n",
    "\n",
    "missing_counts = expected_counts - all_counts\n",
    "\n",
    "print(f\"Missing count values: {sorted(missing_counts)}\")\n",
    "print(len(missing_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20b36d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid rows processed: 1672\n"
     ]
    }
   ],
   "source": [
    "# save processed data file\n",
    "final_df = pd.DataFrame()\n",
    "count = 0\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    for idx, msg in df['messages'].items():\n",
    "        if pd.isna(msg):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            data = json.loads(msg)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Skipping row {idx} due to JSON decoding error.\")\n",
    "            continue\n",
    "        data_df = pd.DataFrame(data)\n",
    "\n",
    "        data_df['transcript_id'] = df.loc[idx, 'conversation_id']\n",
    "\n",
    "        data_df['count'] = df.loc[idx, 'count']\n",
    "\n",
    "        final_df = pd.concat([final_df, data_df], ignore_index=True)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "# fix spelling on participant ID\n",
    "final_df.rename(columns={'participent_id': 'PID'}, inplace=True)\n",
    "\n",
    "# save data file to processed folder\n",
    "save_path = output_folder + \"/\" + \"full_sample_processed.csv\"\n",
    "final_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"Total valid rows processed: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4f0ebe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique annotated transcripts = 1464\n",
      "Mean annotation count per participant = 20.672738312082576\n",
      "Number of participants with fewer than 6 annotations = 153\n"
     ]
    }
   ],
   "source": [
    "# print number of unique transcripts\n",
    "print(f\"Number of unique annotated transcripts = {final_df['transcript_id'].nunique()}\")\n",
    "\n",
    "# make PID a string instead of a list\n",
    "final_df['PID_str'] = final_df['PID'].apply(lambda x: ''.join(map(str, x)))\n",
    "\n",
    "# get count of annotations per PID\n",
    "annotation_count = final_df.groupby('PID_str')['new_topic'].nunique().reset_index()\n",
    "annotation_count.rename(columns = {'new_topic': 'PID_count'}, inplace=True)\n",
    "\n",
    "# print average number of annotations per PID\n",
    "print(f\"Mean annotation count per participant = {annotation_count['PID_count'].mean()}\")\n",
    "\n",
    "# print the number of participants with fewer than 6 annotations\n",
    "print(f\"Number of participants with fewer than 6 annotations = {(annotation_count['PID_count'] < 6).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b998f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcripts not in annotation list (4):\n",
      "43540c29-7c51-4fc8-8bda-fe8614340c64\n",
      "765f6cde-5291-4047-89c1-d71b1e3a413d\n",
      "7a47e9ed-6bfa-499f-811b-f9e2bffc2770\n",
      "bfa73b4c-5b45-4c17-9fe4-8b4f397654e6\n"
     ]
    }
   ],
   "source": [
    "# define directory for CANDOR transcripts\n",
    "folder_dir = '/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/CANDOR/transcripts/raw'\n",
    "# load all modified transcripts and combine into one data frame\n",
    "all_transcripts = []\n",
    "for dirpath, dirnames, filenames in os.walk(folder_dir):\n",
    "    for filename in filenames:\n",
    "        if filename == 'transcript_backbiter_transformed_noLine1.csv':\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            relative_path = os.path.relpath(dirpath, folder_dir)\n",
    "            transcript_id = relative_path.split(os.sep)[0] if relative_path else ''\n",
    "            # add new variable for transcript ID from folder name\n",
    "            df['transcript_id'] = transcript_id\n",
    "            all_transcripts.append(df)\n",
    "\n",
    "# Concatenate all dataframes by rows (like row bind)\n",
    "all_transcripts = pd.concat(all_transcripts, ignore_index=True)\n",
    "\n",
    "# get list of transcript names in the directory\n",
    "folder_names = [name for name in os.listdir(folder_dir) if os.path.isdir(os.path.join(folder_dir, name))]\n",
    "\n",
    "# now get list of transcript IDs from \n",
    "transcript_ids = final_df['transcript_id'].unique()\n",
    "# concatenate dense subset IDs with full sample IDs\n",
    "combined_transcripts = np.concatenate((transcript_ids, dense_transcripts))\n",
    "\n",
    "# convert lists to sets to efficiently compare\n",
    "folder_set = set(folder_names)\n",
    "id_set = set(combined_transcripts)\n",
    "\n",
    "# find transcripts in CANDOR data that are not in the list of IDs in the list\n",
    "unannotated_transcripts = folder_set - id_set\n",
    "\n",
    "# print missing transcripts\n",
    "print(f\"Transcripts not in annotation list ({len(unannotated_transcripts)}):\")\n",
    "for folder in sorted(unannotated_transcripts):\n",
    "    print(folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv-python3-11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
