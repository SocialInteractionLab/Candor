---
title: "(1) Data and Transcript Processing"
author: "Helen Schmidt"
date-modified: today
format: html
toc: true
self-contained: true
---

# Setup
```{r, setup, include = FALSE, message = FALSE}
# set script working directory by checking for user (HS home or lab)
if (Sys.info()[[7]] == "helenschmidt") {
  knitr::opts_knit$set(root.dir = "/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/Conversation-Structure/")
} else if (Sys.info()[[7]] == "tuo70125") {
  knitr::opts_knit$set(root.dir = "/Users/tuo70125/My Drive/SANLab/Experiments/Conversation-Structure/")}

# define CANDOR transcript path
if (Sys.info()[[7]] == "helenschmidt") {
  candor <- "/Users/helenschmidt/Library/CloudStorage/GoogleDrive-helenschmidt129@gmail.com/My Drive/SANLab/Experiments/CANDOR/transcripts/raw"
} else if (Sys.info()[[7]] == "tuo70125") {
  candor <- "/Users/tuo70125/My Drive/SANLab/Experiments/CANDOR/transcripts/raw"}

options(max.print=1000000)
```

```{r, include = FALSE, message = FALSE}
# packages
library(scales)
library(tidyverse)
```

# 1. Merge Raw Data
```{r}
# get all subfolder paths to raw participant data files
raw <- list.files("./data/raw/final",
                  full.names = TRUE,
                  pattern = "*.csv",
                  recursive = FALSE)
# bind data into one data frame
raw <- do.call(rbind, lapply(raw, function(x) { data = read.csv(x, header = TRUE)
                                                return(data) })) #44227
```

# 2. Participant exclusions

```{r}
# rename participant ID variable to PID
names(raw)[6] <- "PID"

# count number of annotated transcripts
length(unique(raw$transcript_id)) #201

# display number of annotations per participant
raw |>
  ungroup() |>
  group_by(PID) |>
  summarize(num_annotations = length(unique(new_topic))) |>
  ggplot(aes(x = num_annotations)) +
  geom_histogram(binwidth = 1) +
  theme_minimal()
```

```{r}
# get quartiles and median annotation value
raw |>
  ungroup() |>
  group_by(PID) |>
  summarize(num_annotations = length(unique(new_topic))) |>
  summarize(Q_05 = quantile(num_annotations, 0.05),
            Q_10 = quantile(num_annotations, 0.1),
            Q_25 = quantile(num_annotations, 0.25),
            Q_50 = quantile(num_annotations, 0.5), # median
            Q_75 = quantile(num_annotations, 0.75),
            Q_90 = quantile(num_annotations, 0.9),
            Q_95 = quantile(num_annotations, 0.95))

# exclude participants who made 6 or fewer or 40 or greater annotations
exclude_by_ann_number <- raw |>
  ungroup() |>
  group_by(PID) |>
  summarize(num_annotations = length(unique(new_topic))) |>
  filter(num_annotations <= 6 | num_annotations >= 40)

# make list of participant IDs to exclude
exclude_by_ann_number <- na.omit(exclude_by_ann_number$PID) #346

# show
exclude_by_ann_number

# print (not including Isaiah test, hence - 1)
paste0("Number of participants excluded due to below/over annotation number thresholds = ", length(exclude_by_ann_number)-1, sep = "")
```

```{r}

## | --------------- ID number --------------- | --------------- reason for exclusion --------------- |
## | [False, 'Isaiah_test', None]              | test ID (included in annotation number exclusion)    |
## |                                           |                                                      |
## | ------------------------------------------------------------------------------------------------ |

# exclusion count
# 345
```

```{r}
# exclude participants from raw data
raw_excluded <- raw |>
  filter(!(PID %in% exclude_by_ann_number))

# print
paste0("Final number of included participants = ", length(na.omit(unique(raw_excluded$PID))), sep = "")
```

# 3. Merge Raw Data w/ Transcripts

```{r}
# get a list of annotated transcripts (minus excluded participants)
annotated_transcripts <- unique(raw_excluded$transcript_id) #200
```

```{r}
# get all subfolder paths to backbiter CANDOR transcripts
backbiter <- list.files(candor,
                        full.names = TRUE,
                        pattern = "transcript_backbiter.csv",
                        recursive = TRUE) #1656

# bind backbiter data into one data frame and add transcript_id from folder name
backbiter <- do.call(rbind, lapply(backbiter, function(x) { data = read.csv(x, header = TRUE)
                                                       convo = str_extract(x, "(?<=raw/)[^/]+")
                                                       data$transcript_id = convo
                                                       return(data)}))

# select only backbiter transcripts that have associated annotations
backbiter_subset <- subset(backbiter, transcript_id %in% annotated_transcripts) #71615
```


```{r}
# merge backbiter transcript data with annotation data (excluded participants already removed)
raw_final <- merge(backbiter_subset, raw_excluded,
                   by = c('transcript_id', 'turn_id'),
                   all = TRUE, sort = TRUE)

# select only some variables of interest from backbiter_subset for transcript tiling
backbiter_subset <- backbiter_subset |>
  select(turn_id, speaker, transcript_id, utterance)

# export raw merged data
write.csv(raw_final, "./data/raw/full_dense_subset_raw.csv", row.names = FALSE)

# save raw_final as df
df <- raw_final

# remove unnecessary data frames
rm(backbiter, raw, annotated_transcripts, candor, raw_final, raw_excluded)
```

# 4. Annotation Checks
```{r}
# print number of conversations
df |> summarize(num_convos = length(na.omit(unique(transcript_id)))) 
# print number of participants
df |> summarize(num_PID = length(na.omit(unique(PID))))
# print number of participants who annotated each transcript
df |>
  select(transcript_id, PID) |>
  dplyr::group_by(transcript_id) |>
  summarize(number_annotators = length(na.omit(unique(PID)))) |>
  arrange(-number_annotators)

# re-scale turn ids so each convo is 0 - 1
df <- df |>
  group_by(transcript_id) |>
  arrange(turn_id, .by_group = TRUE) |>
  mutate(scaled_turn_id = round(rescale(turn_id),2))

# print info
paste0("Final number of annotated backbiter transcripts = ", length(unique(backbiter_subset$transcript_id)),
       sep = "")
```

```{r}
# export backbiter subset for tiling
write.csv(backbiter_subset, "./data/processed/backbiter_subset_for_tiling.csv", row.names = FALSE)
# save main df for jaccard similarity
write.csv(df, "./data/processed/jaccard_data.csv", row.names = FALSE)
# remove
rm(backbiter_subset, exclude_by_ann_number)
```

```{r}
# select only variables of interest to export
df_subset <- df |>
  select(transcript_id, turn_id, scaled_turn_id, PID, utterance, 
         currentUtterance, previous_topic, new_topic)

# export processed data
write.csv(df_subset, "./data/processed/dense_subset_processed.csv", row.names = FALSE)
```

# 5. Topic Label Annotations
```{r, warning = FALSE, message = FALSE}
# export participant topic labels for topic clustering

# loop through each participant's annotations, subset to just the transcript they annotated and then to just their PID to get 1) the order of topics labeled and 2) the length of each topic

# get a list of unique participant IDs
participants <- na.omit(unique(df$PID))
# create data frame to store duplicate transcripts with filled IDs/new_topic labels
all_pid_transcripts <- data.frame()
# create data frame to store topic information for each participant
all_pid_topics <- data.frame()

for (i in participants) {
  # find the transcript_id for this participant
  this_transcript_id <- na.omit(unique(df$transcript_id[df$PID == i]))
  # subset df to just this transcript id
  this_transcript <- subset(df, transcript_id == this_transcript_id)
  # select just transcript-specific variables
  transcript_only <- this_transcript |>
    select(transcript_id, turn_id, scaled_turn_id, speaker.x, utterance) |>
    distinct() |>
    rename(speaker = speaker.x)
  # select participant-specific variables
  PID_only <- this_transcript |>
    select(PID, previous_topic, new_topic, turn_id) |>
    filter(PID == i)
  # add turn 0 to PID only with "Starting the Call" in new_topic
  add <- data.frame(transcript_id = this_transcript_id,
                    PID = i,
                    previous_topic = NA,
                    new_topic = "Starting the Call",
                    turn_id = 0)
  PID_only <- rbind(add, PID_only)
  # bring PID_only and transcript_only back together
  this_transcript <- merge(transcript_only, PID_only, by = c("turn_id", "transcript_id"), all = TRUE)
  # remove previous topic variable
  this_transcript <- this_transcript |>
    select(-previous_topic)
  # fill in new_topic labels across rows
  this_transcript <- this_transcript |> fill(new_topic, .direction = "down")
  # add a topic order number
  this_transcript <- this_transcript |>
    ungroup() |>
    mutate(topic_order = cumsum(new_topic != lag(new_topic, default = first(new_topic))))
  # put PID in all rows
  this_transcript$PID <- i
  # add to all_pid_transcripts
  all_pid_transcripts <- rbind(all_pid_transcripts, this_transcript)
  
  # now that we have that, let's get the number of turns and topic length per participant
  this_pid_summary <- this_transcript |>
    ungroup() |>
    group_by(PID, new_topic, topic_order) |>
    summarize(number_of_turns = length(new_topic)) |>
    ungroup() |>
    arrange(topic_order)
  # add to all_pid_topics
  all_pid_topics <- rbind(all_pid_topics, this_pid_summary)
}

# export data frames
write.csv(all_pid_topics, "./data/processed/all_participant_topic_labels.csv", row.names = FALSE)
write.csv(all_pid_transcripts, "./data/processed/all_participant_transcripts.csv", row.names = FALSE)

```

```{r}
# average topic length (turns) per participant
all_pid_topics |>
  dplyr::group_by(PID) |>
  summarize(mean_topic_length_turns = mean(number_of_turns)) |>
  ggplot(aes(x = mean_topic_length_turns)) +
  geom_histogram() +
  theme_minimal()
```

# 6. Average conversation length & number of utterances
```{r}
# average conversation length
df |>
  select(transcript_id, turn_id, start, stop) |>
  mutate(turn_length = as.numeric(stop) - as.numeric(start)) |>
  select(transcript_id, turn_id, turn_length) |>
  distinct() |>
  group_by(transcript_id) |>
  summarize(turn_count = max(turn_id),
            total_length = sum(turn_length)) |>
  ungroup() |>
  summarize(average_turn_count = mean(turn_count),
            sd_turn_count = sd(turn_count),
            average_total_length = mean(total_length)/60,
            sd_total_length = sd(total_length)/60)
```

